{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5e090c3c",
      "metadata": {
        "id": "5e090c3c"
      },
      "source": [
        "# Tokenization Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "930c0a94",
      "metadata": {
        "id": "930c0a94"
      },
      "source": [
        "### Sample Text:\n",
        "```python\n",
        "sample_text = \"Artificial Intelligence is revolutionizing the world of technology.\"\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "char_tokens = list(sample_text)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "FrobZYwEpN9c"
      },
      "id": "FrobZYwEpN9c"
    },
    {
      "cell_type": "markdown",
      "id": "d7460a21",
      "metadata": {
        "id": "d7460a21"
      },
      "source": [
        "## 1. Character Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9eb2d643",
      "metadata": {
        "id": "9eb2d643"
      },
      "source": [
        "Tokenize the sample text into individual characters.\n",
        "\n",
        "Write your implementation below:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"Artificial Intelligence is revolutionizing the world of technology.\""
      ],
      "metadata": {
        "id": "pflwLt9RpgOO"
      },
      "id": "pflwLt9RpgOO",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "275d764f",
      "metadata": {
        "id": "275d764f",
        "outputId": "4322ea88-f10f-4781-f4d2-d5df941ec9ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A', 'r', 't', 'i', 'f', 'i', 'c', 'i', 'a', 'l', ' ', 'I', 'n', 't', 'e', 'l', 'l', 'i', 'g', 'e', 'n', 'c', 'e', ' ', 'i', 's', ' ', 'r', 'e', 'v', 'o', 'l', 'u', 't', 'i', 'o', 'n', 'i', 'z', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 'w', 'o', 'r', 'l', 'd', ' ', 'o', 'f', ' ', 't', 'e', 'c', 'h', 'n', 'o', 'l', 'o', 'g', 'y', '.']\n"
          ]
        }
      ],
      "source": [
        "char_tokens = list(sample_text)\n",
        "print(char_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a3a922a",
      "metadata": {
        "id": "0a3a922a"
      },
      "source": [
        "## 2. Word Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2c596b3",
      "metadata": {
        "id": "c2c596b3"
      },
      "source": [
        "Tokenize the sample text into words using the `nltk` library.\n",
        "\n",
        "Write your implementation below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d12be75f",
      "metadata": {
        "id": "d12be75f",
        "outputId": "11f6b2f0-4655-4ead-e0bf-a9c7be71977d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Artificial', 'Intelligence', 'is', 'revolutionizing', 'the', 'world', 'of', 'technology', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "word_tokens = word_tokenize(sample_text)\n",
        "print(word_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86fc94db",
      "metadata": {
        "id": "86fc94db"
      },
      "source": [
        "## 3. Sentence Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae693c20",
      "metadata": {
        "id": "ae693c20"
      },
      "source": [
        "Tokenize the following paragraph into sentences:\n",
        "```python\n",
        "paragraph = \"Tokenization is essential. It breaks text into meaningful units. This allows for easier processing.\"\n",
        "```\n",
        "\n",
        "Use the `nltk` library for sentence tokenization. Write your implementation below:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"Token is essential. It break text into meaning unit. This allow for easy process.\""
      ],
      "metadata": {
        "id": "sjtLdPfvqb_F"
      },
      "id": "sjtLdPfvqb_F",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e5243859",
      "metadata": {
        "id": "e5243859",
        "outputId": "994ac1b2-1e4a-47eb-d1b9-ae8c04f9ba2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Token is essential.', 'It break text into meaning unit.', 'This allow for easy process.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "sent_tokens = sent_tokenize(paragraph)\n",
        "print(sent_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7739f726",
      "metadata": {
        "id": "7739f726"
      },
      "source": [
        "## 4. Byte Pair Encoding (BPE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a016010",
      "metadata": {
        "id": "3a016010"
      },
      "source": [
        "Use the `tokenizers` library to apply Byte Pair Encoding (BPE) to the sample text.\n",
        "\n",
        "Write your implementation below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "abfd6076",
      "metadata": {
        "id": "abfd6076",
        "outputId": "0f9e84ad-56c6-4b0a-8e02-f8e9c8e44958",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Token', 'i', 'a', 't', 'i', 'o', 'n', ' is essential', '. It brea', 'k', 's ', 't', 'ext i', 'nto m', 'eani', 'n', 'g', 'f', 'u', 'l', ' u', 'ni', 't', 's', '. ', 'This ', 'allo', 'w', 's ', 'f', 'or', ' ea', 's', 'i', 'e', 'r', ' process', 'i', 'n', 'g', '.']\n"
          ]
        }
      ],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "tokenizer = Tokenizer(BPE())\n",
        "trainer = BpeTrainer(vocab_size=1000)\n",
        "tokenizer.pre_tokenizers = Whitespace()\n",
        "\n",
        "paragraph = [\"Token is essential. It break text into meaning unit. This allow for easy process.\"]\n",
        "tokenizer.train_from_iterator(paragraph, trainer)\n",
        "\n",
        "output = tokenizer.encode(\"Tokenization is essential. It breaks text into meaningful units. This allows for easier processing.\")\n",
        "print(output.tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b_ksHe2yuxjC"
      },
      "id": "b_ksHe2yuxjC",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}